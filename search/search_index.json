{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Benchmark utils","text":"<p>Utils for benchmark - wrapper over python timeit.</p> <p> </p> <p>Tested on python 3.8 - 3.12</p>"},{"location":"#install","title":"Install","text":"<p>Install from pypi:  </p> <p><code>pip install benchmark_utils</code></p> <p>Or install from github repo:</p> <p><code>pip install git+https://github.com/ayasyrev/benchmark_utils.git</code></p>"},{"location":"#basic-use","title":"Basic use.","text":"<p>Lets benchmark some (dummy) functions.</p> <pre><code>from time import sleep\n\n\ndef func_to_test_1(sleep_time: float = 0.1, mult: int = 1) -&gt; None:\n    \"\"\"simple 'sleep' func for test\"\"\"\n    sleep(sleep_time * mult)\n\n\ndef func_to_test_2(sleep_time: float = 0.11, mult: int = 1) -&gt; None:\n    \"\"\"simple 'sleep' func for test\"\"\"\n    sleep(sleep_time * mult)\n</code></pre> <p>Let's create benchmark.</p> <pre><code>from benchmark_utils import Benchmark\n</code></pre> <pre><code>bench = Benchmark(\n    [func_to_test_1, func_to_test_2],\n)\n</code></pre> <pre><code>bench\n</code></pre> output <pre>Benchmark(func_to_test_1, func_to_test_2)</pre> <p>Now we can benchmark that functions.</p> <pre><code># we can run bench.run() or just:\nbench()\n</code></pre> output <pre> Func name  | Sec / run\n</pre> <pre>func_to_test_1:   0.10 0.0%\n</pre> <pre>func_to_test_2:   0.11 -9.1%\n</pre> <p>We can run it again, all functions, some of it, exclude some and change number of repeats.</p> <pre><code>bench.run(num_repeats=10)\n</code></pre> output <pre></pre> <pre>\n</pre> <pre> Func name  | Sec / run\n</pre> <pre>func_to_test_1:   0.10 0.0%\n</pre> <pre>func_to_test_2:   0.11 -9.1%\n</pre> <p>After run, we can print results - sorted or not, reversed, compare results with best or not. </p> <pre><code>bench.print_results(reverse=True)\n</code></pre> output <pre> Func name  | Sec / run\n</pre> <pre>func_to_test_2:   0.11 0.0%\n</pre> <pre>func_to_test_1:   0.10 10.0%\n</pre> <p>We can add functions to benchmark as list of functions (or partial) or as dictionary: <code>{\"name\": function}</code>.</p> <pre><code>bench = Benchmark(\n    [\n        func_to_test_1,\n        partial(func_to_test_1, 0.12),\n        partial(func_to_test_1, sleep_time=0.11),\n    ]\n)\n</code></pre> <pre><code>bench\n</code></pre> output Benchmark(func_to_test_1, func_to_test_1(0.12), func_to_test_1(sleep_time=0.11))<pre>\n\n\n<pre><code>bench.run()\n</code></pre>\n output\n<pre> Func name  | Sec / run\n</pre>\n\n\n\n\n<pre>func_to_test_1:   0.10 0.0%\n</pre>\n\n\n\n\n<pre>func_to_test_1(sleep_time=0.11):   0.11 -9.1%\n</pre>\n\n\n\n\n<pre>func_to_test_1(0.12):   0.12 -16.7%\n</pre>\n\n\n<pre><code>bench = Benchmark(\n    {\n        \"func_1\": func_to_test_1,\n        \"func_2\": func_to_test_2,\n    }\n)\n</code></pre>\n<pre><code>bench\n</code></pre>\n output  \n<pre>Benchmark(func_1, func_2)</pre>\n\n\n<p>When we run benchmark script in terminal, we got pretty progress thanks to rich. Lets run example_1.py from example folder:</p>\n<p></p>"},{"location":"#benchmarkiter","title":"BenchmarkIter","text":"<p>With BenchmarkIter we can benchmark functions over iterables, for example read list of files or run functions with different arguments.</p>\n<pre><code>def func_to_test_1(x: int) -&gt; None:\n    \"\"\"simple 'sleep' func for test\"\"\"\n    sleep(0.01)\n\n\ndef func_to_test_2(x: int) -&gt; None:\n    \"\"\"simple 'sleep' func for test\"\"\"\n    sleep(0.015)\n\n\ndummy_params = list(range(10))\n</code></pre>\n<pre><code>from benchmark_utils import BenchmarkIter\n\nbench = BenchmarkIter(\n    func=[func_to_test_1, func_to_test_2],\n    item_list=dummy_params,\n)\n</code></pre>\n<pre><code>bench()\n</code></pre>\n output\n<pre> Func name  | Items/sec\n</pre>\n\n\n\n\n<pre>func_to_test_1:  97.93\n</pre>\n\n\n\n\n<pre>func_to_test_2:  65.25\n</pre>\n\n\n<p>We can run it again, all functions, some of it, exclude some and change number of repeats.\nAnd we can limit number of items with <code>num_samples</code> argument:\n<code>bench.run(num_samples=5)</code></p>"},{"location":"#multiprocessing","title":"Multiprocessing","text":"<p>By default we tun functions in one thread.\nBut we can use multiprocessing with <code>multiprocessing=True</code> argument:\n<code>bench.run(multiprocessing=True)</code>\nIt will use all available cpu cores.\nAnd we can use <code>num_workers</code> argument to limit used cpu cores:\n<code>bench.run(multiprocessing=True, num_workers=2)</code></p>\n<pre><code>bench.run(multiprocessing=True, num_workers=2)\n</code></pre>\n output\n\n<pre> Func name  | Items/sec\n</pre>\n\n\n\n\n<pre>func_to_test_1: 173.20\n</pre>\n\n\n\n\n<pre>func_to_test_2: 120.80\n</pre>"}]}